# ğŸ”„ Zaksha Service Separation Guide

## ğŸ“‹ **Current Status**

We have successfully created a microservices architecture with two separate repositories:

### **1. Main App Repository** (`zaksha-web-mvp`)
- **Location**: `/Users/tarunsnehithkishorereddykarna/Downloads/zaksha-web-mvp`
- **Type**: Next.js Frontend Application
- **Purpose**: UI components, user interface, core features
- **Port**: 3000 (or 3002 if 3000 is busy)

### **2. Interview AI Microservice** (`zaksha-interview-ai`)
- **Location**: `/Users/tarunsnehithkishorereddykarna/Downloads/zaksha-interview-ai`
- **Type**: Express.js + TypeScript Microservice
- **Purpose**: Fraud detection, LLM analysis, video platform
- **Port**: 3001 (or 3003 to avoid conflicts)

## ğŸ—ï¸ **Architecture Overview**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ZAKSHA PLATFORM                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP/WebSocket    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Main App      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Interview AI    â”‚ â”‚
â”‚  â”‚  (Frontend)     â”‚                      â”‚  Microservice   â”‚ â”‚
â”‚  â”‚                 â”‚                      â”‚                 â”‚ â”‚
â”‚  â”‚ - UI Components â”‚                      â”‚ - Fraud Detectionâ”‚ â”‚
â”‚  â”‚ - User Auth     â”‚                      â”‚ - LLM Analysis  â”‚ â”‚
â”‚  â”‚ - Core Features â”‚                      â”‚ - Video Platformâ”‚ â”‚
â”‚  â”‚ - Port 3000     â”‚                      â”‚ - Port 3001     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ **How to Run Both Services**

### **Step 1: Start the Microservice**
```bash
# Navigate to microservice directory
cd /Users/tarunsnehithkishorereddykarna/Downloads/zaksha-interview-ai

# Install dependencies (if not already done)
npm install

# Set up environment
./setup-env.sh

# Start the microservice
npm run dev
# This will run on http://localhost:3001
```

### **Step 2: Start the Main App**
```bash
# Navigate to main app directory
cd /Users/tarunsnehithkishorereddykarna/Downloads/zaksha-web-mvp

# Set up microservice integration
./setup-microservice-integration.sh

# Start the main app
npm run dev
# This will run on http://localhost:3000
```

## ğŸ”§ **Service Features**

### **Main App (zaksha-web-mvp)**
- âœ… Next.js frontend application
- âœ… UI components and pages
- âœ… User authentication
- âœ… Dashboard and core features
- âœ… API proxy to microservice
- âœ… Environment configuration

### **Interview AI Microservice (zaksha-interview-ai)**
- âœ… Express.js + TypeScript backend
- âœ… Fraud detection algorithms
- âœ… LLM integration (OpenAI GPT-4)
- âœ… Video platform (WebRTC)
- âœ… WebSocket support
- âœ… RESTful API endpoints
- âœ… Health monitoring
- âœ… Environment configuration

## ğŸ“¡ **API Endpoints**

### **Main App (Proxy)**
- `POST /api/fraud/start` â†’ Proxies to microservice
- `POST /api/llm/analyze` â†’ Proxies to microservice
- `POST /api/llm/questions` â†’ Proxies to microservice

### **Microservice (Direct)**
- `GET /health` â†’ Health check
- `POST /api/v1/fraud/start` â†’ Start interview
- `POST /api/v1/fraud/end/:sessionId` â†’ End interview
- `POST /api/v1/fraud/transcript` â†’ Submit transcript
- `POST /api/v1/fraud/screen` â†’ Submit screen events
- `GET /api/v1/fraud/score/:sessionId` â†’ Get fraud score
- `POST /api/v1/llm/analyze` â†’ LLM analysis
- `POST /api/v1/llm/questions` â†’ Generate questions
- `GET /api/v1/video/config` â†’ Video configuration

## ğŸ”‘ **Environment Setup**

### **Microservice (.env)**
```bash
PORT=3001
NODE_ENV=development
CORS_ORIGIN=http://localhost:3000
OPENAI_API_KEY=sk-your-openai-api-key-here
LLM_MODEL=gpt-4
ENABLE_LLM_ANALYSIS=true
ENABLE_VIDEO_CALLS=true
ENABLE_FRAUD_DETECTION=true
```

### **Main App (.env.local)**
```bash
INTERVIEW_AI_SERVICE_URL=http://localhost:3001
INTERVIEW_AI_WS_URL=http://localhost:3001
OPENAI_API_KEY=sk-your-openai-api-key-here
```

## ğŸ§ª **Testing the Services**

### **1. Test Microservice Health**
```bash
curl http://localhost:3001/health
```

### **2. Test Main App Health**
```bash
curl http://localhost:3000/health
```

### **3. Test Interview Start**
```bash
curl -X POST http://localhost:3000/api/fraud/start \
  -H "Content-Type: application/json" \
  -d '{
    "candidateId": "test123",
    "companyId": "company456",
    "consent": {
      "audio": true,
      "video": true,
      "screen": true,
      "telemetry": true
    }
  }'
```

## ğŸš¨ **Troubleshooting**

### **Port Conflicts**
If you get port conflicts:
1. Kill existing processes: `pkill -f "next dev" && pkill -f "nodemon"`
2. Update ports in environment files
3. Restart services

### **API Key Issues**
1. Get OpenAI API key from: https://platform.openai.com/api-keys
2. Add to microservice `.env` file
3. Restart microservice

### **Connection Issues**
1. Ensure both services are running
2. Check environment variables
3. Verify CORS settings

## ğŸ“ **Repository Structure**

### **Main App (zaksha-web-mvp)**
```
zaksha-web-mvp/
â”œâ”€â”€ app/                    # Next.js app directory
â”œâ”€â”€ components/             # React components
â”œâ”€â”€ lib/                    # Utilities and services
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ interview-ai.ts # Microservice client
â”œâ”€â”€ package.json
â”œâ”€â”€ .env.local             # Environment variables
â””â”€â”€ setup-microservice-integration.sh
```

### **Microservice (zaksha-interview-ai)**
```
zaksha-interview-ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts           # Main server file
â”‚   â”œâ”€â”€ types/             # TypeScript types
â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”œâ”€â”€ fraudDetection.ts
â”‚   â”‚   â”œâ”€â”€ llm.ts
â”‚   â”‚   â””â”€â”€ video.ts
â”‚   â””â”€â”€ routes/            # API routes
â”‚       â”œâ”€â”€ fraud.ts
â”‚       â”œâ”€â”€ llm.ts
â”‚       â””â”€â”€ video.ts
â”œâ”€â”€ package.json
â”œâ”€â”€ .env                   # Environment variables
â””â”€â”€ setup-env.sh
```

## ğŸ¯ **Next Steps**

1. **Set up API keys** in the microservice
2. **Test both services** independently
3. **Integrate services** through the proxy
4. **Deploy to production** when ready
5. **Add monitoring** and logging

## ğŸš€ **Production Deployment**

### **Docker Deployment**
```bash
# Microservice
cd zaksha-interview-ai
docker build -t zaksha-interview-ai .
docker run -p 3001:3001 zaksha-interview-ai

# Main app
cd zaksha-web-mvp
docker build -t zaksha-web-mvp .
docker run -p 3000:3000 zaksha-web-mvp
```

### **Environment Variables for Production**
```bash
# Microservice
NODE_ENV=production
PORT=3001
CORS_ORIGIN=https://your-frontend-domain.com
OPENAI_API_KEY=your-production-api-key

# Main app
INTERVIEW_AI_SERVICE_URL=https://your-microservice-domain.com
INTERVIEW_AI_WS_URL=wss://your-microservice-domain.com
```

---

**ğŸ‰ You now have a fully separated microservices architecture!**

The services are completely independent and can be developed, deployed, and scaled separately.

